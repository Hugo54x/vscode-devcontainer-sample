# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME         - Docker image name used to run Airflow.
#                              Default: apache/airflow:master-python3.8
# AIRFLOW_UID                - User ID in Airflow containers
#                              Default: 50000
# AIRFLOW_GID                - Group ID in Airflow containers
#                              Default: 50000
# _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account.
#                              Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account.
#                              Default: airflow
#
# Feel free to modify this file to suit your needs.

version: '3'
    x-airflow-common:
    &airflow-common
    image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.0.1}
    environment:
        &airflow-common-env
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
        AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
        AIRFLOW__CELERY__BROKER_URL: redis://:@redis:3079/0
        AIRFLOW__CORE__FERNET_KEY: ''
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
        AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    volumes:
        - ./airflow/dags:/opt/airflow/dags
        - ./airflow/logs:/opt/airflow/logs
        - ./airflow/plugins:/opt/airflow/plugins
    user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
    depends_on:
        redis:
        condition: service_healthy
        postgres:
        condition: service_healthy
# SINGLE SERVICE BEGIN
services:
    python:
        build: .
        container_name: "Python_3.9"
        restart: unless-stopped
        init: true
        volumes:
        # You can switch between :cached and :delegated here  
            - ..:/workspace:delegated
        # If volume is used
        #    - workspace-volume:/workspace
        # Run Docker from Docker
            - /var/run/docker.sock:/var/run/docker-host.sock
        ports: 
            - 3020:6006
            - 3021:8500
            - 3022:8501
        command: sleep infinity
    fastapi:
        image: tiangolo/uvicorn-gunicorn-fastapi:python3.8
        container_name: "FastAPI"
        restart: unless-stopped
        environment: 
            VARIABLE_NAME: api
            WORKERS_PER_CORE: 0.5
            MAX_WORKERS: 24
        volumes:
            - ../api/src:/app
        ports:  
            - 3024:80
        links: 
            - "database:db"
        entrypoint: /start-reload.sh
        command: /bin/sh -c "pip3 install --user -r requirements.txt"
    database:
        image: mysql:5.7
        container_name: "MySQL_5.7"
        restart: unless-stopped
        environment: 
            MYSQL_DATABASE: exampledb
            MYSQL_USER: exampleuser
            MYSQL_PASSWORD: examplepass
            MYSQL_RANDOM_ROOT_PASSWORD: '1'
        volumes:
            - ../db:/var/lib/mysql
    tensorflow:
        image: jupyter/tensorflow-notebook
        container_name: "Tensorflow"
        restart: unless-stopped
        ports: 
            - 3026:8888
            - 3027:6006
        volumes:
            - ../app:/home/jovyan/work
        environment:
            JUPYTER_ENABLE_LAB: "yes"
            GRANT_SUDO: "yes"
            RESTARTABLE: "yes"
        entrypoint: start-notebook.sh --NotebookApp.password='' --NotebookApp.token=''
    nodered:
    # there is also a node-red:latest-minimal version availible
        image: node-red:latest
        container_name: "NodeRED"
        restart: unless-stopped
        ports:
            - 1880:1880
        volumes:
            - ../nodeRED:/data
    mosqitto:
        image: eclipse-mosquitto:latest
        container_name: "MQTT"
        restart: unless-stopped
        ports:
            - 1883:1883
            - 9001:9001
        volumes:
            - ../mqtt/config:/mosquitto/config/
            - ../mqtt/data:/mosquitto/data/
            - ../mqtt/logs:/mosquitto/logs/
    # swagger-editor:
    #   image: swaggerapi/swagger-editor
    #   container_name: "Swagger Editor"
    #   restart: unless-stopped
    #   ports:
    #     - "3028:8080"
    #   volumes:
    #     - ../api/specs/:/tmp
    #   environment:
    #     - SWAGGER_FILE=/tmp/swaggei-sneaks.yaml
# SINGLE SERVICE END
# APACHE AIRFLOW BEGIN
  postgres:
    image: postgres:13
    environment:
        POSTGRES_USER: airflow
        POSTGRES_PASSWORD: airflow
        POSTGRES_DB: airflow
    volumes:
        - ./airflow/db:/var/lib/postgresql/data
        # - postgres-db-volume::/var/lib/postgresql/data
    healthcheck:
        test: ["CMD", "pg_isready", "-U", "airflow"]
        interval: 5s
        retries: 5
    restart: unless-stopped

  redis:
    image: redis:latest
    ports:
        - 3079:6379
    healthcheck:
        test: ["CMD", "redis-cli", "ping"]
        interval: 5s
        timeout: 30s
        retries: 50
    restart: unless-stopped

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
        - 3080:8080
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:3080/health"]
        interval: 10s
        timeout: 10s
        retries: 5
    restart: unless-stopped

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: unless-stopped

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    command: version
    environment:
        <<: *airflow-common-env
        _AIRFLOW_DB_UPGRADE: 'true'
        _AIRFLOW_WWW_USER_CREATE: 'true'
        _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
        _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

  flower:
    <<: *airflow-common
    command: celery flower
    ports:
        - 3055:5555
    healthcheck:
        test: ["CMD", "curl", "--fail", "http://localhost:3055/"]
        interval: 10s
        timeout: 10s
        retries: 5
    restart: unless-stopped
# APACHE AIRFLOW END

# volumes:
#     workspace-volume:
#     postgres-db-volume: # APACHE AIRFLOW
